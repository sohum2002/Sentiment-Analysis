{
 "metadata": {
  "name": "",
  "signature": "sha256:8abc4e299627466c840c152d1a8ee445626f07026a6eca856ed60a33931eb9d8"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "from pandas import DataFrame as df\n",
      "from random import shuffle\n",
      "import numpy as np\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
      "from sklearn.feature_extraction.text import TfidfTransformer\n",
      "from sklearn.linear_model import SGDClassifier\n",
      "from sklearn.pipeline import Pipeline\n",
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING: pylab import has clobbered these variables: ['shuffle']\n",
        "`%matplotlib` prevents importing * from pylab and numpy\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "usedColumns = ['Sentiment', 'SentimentText']\n",
      "\n",
      "database = pd.read_csv(\"Sentiment Analysis Dataset.csv\", dtype=\"string\")\n",
      "tweets_db_df = df(database,columns=usedColumns)\n",
      "tweets_db_list = tweets_db_df.values.tolist()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IOError",
       "evalue": "File Sentiment Analysis Dataset.csv does not exist",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-6-fff697da1b6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0musedColumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Sentiment'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SentimentText'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdatabase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sentiment Analysis Dataset.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtweets_db_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0musedColumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtweets_db_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets_db_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/sohumsachdev/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, na_fvalues, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols, infer_datetime_format)\u001b[0m\n\u001b[1;32m    450\u001b[0m                     infer_datetime_format=infer_datetime_format)\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/sohumsachdev/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/sohumsachdev/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_options_with_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/sohumsachdev/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/sohumsachdev/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1041\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/sohumsachdev/anaconda/lib/python2.7/site-packages/pandas/parser.so\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:3218)\u001b[0;34m()\u001b[0m\n",
        "\u001b[0;32m/Users/sohumsachdev/anaconda/lib/python2.7/site-packages/pandas/parser.so\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:5594)\u001b[0;34m()\u001b[0m\n",
        "\u001b[0;31mIOError\u001b[0m: File Sentiment Analysis Dataset.csv does not exist"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "shuffle(tweets_db_list)\n",
      "\n",
      "divider = int(len(tweets_db_list) * 0.7)\n",
      "\n",
      "train = tweets_db_list[:divider]\n",
      "test = tweets_db_list[divider:]\n",
      "    \n",
      "train_x = [i[1].decode(\"utf-8\", \"replace\") for i in train]\n",
      "test_x = [i[1].decode(\"utf-8\", \"replace\") for i in test]\n",
      "\n",
      "train_y = [i[0] for i in train]\n",
      "test_y = [i[0] for i in test]\n",
      "\n",
      "train_y = np.asarray(train_y)\n",
      "test_y = np.asarray(test_y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Multinomial Naive Bayes\n",
      "\n",
      "Form of naive bayes where it takes into account frequency of a word"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Index of word is linked to its' frequency\n",
      "count_vect = CountVectorizer()\n",
      "X_train_counts = count_vect.fit_transform(train_x)\n",
      "\n",
      "#downscale weights for words that occur in many documents in the corpus and are \n",
      "#therefore less informative than those that occur only in a smaller portion of the corpus \n",
      "\n",
      "tfidf_transformer = TfidfTransformer()\n",
      "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
      "clf = MultinomialNB().fit(X_train_tfidf, train_y)\n",
      "\n",
      "X_new_counts = count_vect.transform(test_x)\n",
      "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
      "\n",
      "predicted = clf.predict(X_new_tfidf)\n",
      "\n",
      "np.mean(predicted == test_y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "0.76829225648736543"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Bernoulli Naive Bayes  \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "count_vect = CountVectorizer()\n",
      "X_train_counts = count_vect.fit_transform(train_x)\n",
      "\n",
      "#downscale weights for words that occur in many documents in the corpus and are \n",
      "#therefore less informative than those that occur only in a smaller portion of the corpus \n",
      "\n",
      "tfidf_transformer = TfidfTransformer()\n",
      "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
      "clf = BernoulliNB().fit(X_train_tfidf, train_y)\n",
      "\n",
      "X_new_counts = count_vect.transform(test_x)\n",
      "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
      "\n",
      "predicted = clf.predict(X_new_tfidf)\n",
      "\n",
      "np.mean(predicted == test_y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "0.77344209452178037"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#SGD"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text_clf = Pipeline([('vect', CountVectorizer()),\n",
      "                      ('tfidf', TfidfTransformer()),\n",
      "                      ('clf', SGDClassifier())])\n",
      "\n",
      "text_clf = text_clf.fit(train_x, train_y)\n",
      "predicted = text_clf.predict(test_x)\n",
      "np.mean(predicted == test_y)  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "0.76933176083134913"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#My dataset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "colsUsed = ['Semantic Score (-4 to 4)', 'Text']\n",
      "\n",
      "data = pd.read_csv('@BarackObama1000/Sentimental Analysis.csv')\n",
      "data_df = df(data, columns=colsUsed)\n",
      "\n",
      "# data_df.head()\n",
      "\n",
      "data_df_list = data_df.values.tolist()\n",
      "\n",
      "\n",
      "fixed_data_df_list = []\n",
      "\n",
      "for i in range(len(data_df_list)):\n",
      "    if data_df_list[i][0] < 0:\n",
      "        fixed_data_df_list.append([-1, data_df_list[i][1]])\n",
      "    elif data_df_list[i][0] > 0:\n",
      "        fixed_data_df_list.append([1, data_df_list[i][1]])\n",
      "    elif data_df_list[i][0] == 0:\n",
      "        fixed_data_df_list.append([0, data_df_list[i][1]])\n",
      "\n",
      "shuffle(fixed_data_df_list)\n",
      "\n",
      "divider1 = int(len(fixed_data_df_list) * 0.6)\n",
      "\n",
      "train1 = fixed_data_df_list[:divider1]\n",
      "test1 = fixed_data_df_list[divider1:]\n",
      "\n",
      "train_x1 = [str(i[1]).decode('utf-8', 'ignore') for i in train1]\n",
      "test_x1 = [str(i[1]).decode('utf-8', 'ignore') for i in test1]\n",
      "\n",
      "train_y1 = [i[0] for i in train1]\n",
      "test_y1 = [i[0] for i in test1]\n",
      "\n",
      "train_y1 = np.asarray(train_y1)\n",
      "test_y1 = np.asarray(test_y1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Bayesian"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Index of word is linked to its' frequency\n",
      "count_vect1 = CountVectorizer()\n",
      "X_train_counts1 = count_vect1.fit_transform(train_x1)\n",
      "\n",
      "#downscale weights for words that occur in many documents in the corpus and are \n",
      "#therefore less informative than those that occur only in a smaller portion of the corpus \n",
      "\n",
      "tfidf_transformer1 = TfidfTransformer()\n",
      "X_train_tfidf1 = tfidf_transformer1.fit_transform(X_train_counts1)\n",
      "clf1 = MultinomialNB().fit(X_train_tfidf1, train_y1)\n",
      "\n",
      "X_new_counts1 = count_vect1.transform(test_x1)\n",
      "X_new_tfidf1 = tfidf_transformer1.transform(X_new_counts1)\n",
      "\n",
      "predicted1 = clf1.predict(X_new_tfidf1)\n",
      "\n",
      "np.mean(predicted1 == test_y1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "0.75362318840579712"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Bernoulli Naive Bayes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Index of word is linked to its' frequency\n",
      "count_vect1 = CountVectorizer()\n",
      "X_train_counts1 = count_vect1.fit_transform(train_x1)\n",
      "\n",
      "#downscale weights for words that occur in many documents in the corpus and are \n",
      "#therefore less informative than those that occur only in a smaller portion of the corpus \n",
      "\n",
      "tfidf_transformer1 = TfidfTransformer()\n",
      "X_train_tfidf1 = tfidf_transformer1.fit_transform(X_train_counts1)\n",
      "clf1 = BernoulliNB().fit(X_train_tfidf1, train_y1)\n",
      "\n",
      "X_new_counts1 = count_vect1.transform(test_x1)\n",
      "X_new_tfidf1 = tfidf_transformer1.transform(X_new_counts1)\n",
      "\n",
      "predicted1 = clf1.predict(X_new_tfidf1)\n",
      "\n",
      "np.mean(predicted1 == test_y1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "0.75362318840579712"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#SGD"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text_clf = Pipeline([('vect', CountVectorizer()),\n",
      "                      ('tfidf', TfidfTransformer()),\n",
      "                      ('clf', SGDClassifier())])\n",
      "\n",
      "text_clf = text_clf.fit(train_x1, train_y1)\n",
      "predicted1 = text_clf.predict(test_x1)\n",
      "np.mean(predicted1 == test_y1)  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "0.76086956521739135"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}